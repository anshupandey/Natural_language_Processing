{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec model with gensim IMDB reviews dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Natural_language_Processing/blob/master/word2vec_model_with_gensim_IMDB_reviews_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyc_0lfv98xf"
      },
      "source": [
        "# The data\n",
        "\n",
        "   ##  About the data\n",
        "The analysis seeks to establish transformation of word into vectors on any text. We are not concerned about whether the text data has label or not. The data set supplied consists of  **50000 IMDB reviews**  with review ID on a certain movie  with no labels.We'll use this unlabelled data to train a model. which can be applied on test data.\n",
        "\n",
        "Please visit the site to download the data\n",
        "https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qjeipfq-Xvy"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUO7x4uTqEyv"
      },
      "source": [
        "## Import the data\n",
        "\n",
        "The data was imported from local repository using the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurxAg-o7Oe3"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/0ygoimffauvl7x5/unlabeledTrainData.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb2PtvNs-ezJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358ee8d7-3286-428a-dd9a-ccdf012f504e"
      },
      "source": [
        "df=pd.read_csv(\"unlabeledTrainData.tsv\",delimiter=\"\\t\",quoting=3,header=0)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTir5Bc-e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6c2bfe4e-9908-4061-cf19-0397ec3fc969"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                             review\n",
              "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
              "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
              "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
              "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
              "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCGqHLWu-e7m"
      },
      "source": [
        "import re,string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEa71UBQe9X7"
      },
      "source": [
        "##  Data Cleaning\n",
        "We've gone through the reviews & detected punctuations in many reviews.The punctuations don't contribute anything to our analysis & moreover they are considered as unique word & distort the meaning of other words.This is why the data needs to be cleaned before we jump into core analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWxkM_PZ-e-h"
      },
      "source": [
        "def clean_string(string):                                                         # The entire document is cleaned defining clean_string\n",
        "  try:\n",
        "    string=re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*','',string,flags=re.MULTILINE) # remove URLS\n",
        "    string=re.sub(r\"[^A-Za-z]\",\" \",string) # remove non alphabetic tokens\n",
        "    words=string.strip().lower().split() # removing extra space\n",
        "    return \" \".join(words)\n",
        "  except:\n",
        "    return \" \"\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWcGFms8a9TK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwzVmg29asKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9240da1c-de0d-455a-97fb-e11d688bc584"
      },
      "source": [
        "doc = \"my name is anshu 123anshu http://www.anshi.com jeio\"\n",
        "re.sub(r\"[^A-Za-z]\",\" \",doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my name is anshu    anshu http   www anshi com jeio'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrc9brjJyEP"
      },
      "source": [
        "Above we defined a function called **clean_string** & this function we have applied on the raw review column and created a new column(**clean_review**) to save the cleaned reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rZpIB7i-fBK"
      },
      "source": [
        "df['clean_review']=df.review.apply(clean_string)                                  # Finally cleaned format is applied on the reviews\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrIOSWx3-fFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "39a24aa9-ba35-4014-c1b8-29cce55451a0"
      },
      "source": [
        "print (\"No.of samples \\n:\",(len(df)))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No.of samples \n",
            ": 50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "      <td>watching time chasers it obvious that it was m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "      <td>i saw this film about years ago and remember i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "      <td>minor spoilers br br in new york joan barnard ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "      <td>i went to see this film with a great deal of e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "      <td>yes i agree with everyone on this site this mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                       clean_review\n",
              "0   \"9999_0\"  ...  watching time chasers it obvious that it was m...\n",
              "1  \"45057_0\"  ...  i saw this film about years ago and remember i...\n",
              "2  \"15561_0\"  ...  minor spoilers br br in new york joan barnard ...\n",
              "3   \"7161_0\"  ...  i went to see this film with a great deal of e...\n",
              "4  \"43971_0\"  ...  yes i agree with everyone on this site this mo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWVMOiItP2L"
      },
      "source": [
        "If we look at the data now, we'll not notice any punctuations in the **clean_review** column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcKWqrLo2ZRz"
      },
      "source": [
        "#  Word2Vec with Gensim(The Word2Vec toolkit)\n",
        "\n",
        "Gensim is an open source Python library for natural language processing, with a focus on topic modeling.Gensim was developed and is maintained by the Czech natural language processing researcher **Radim Řehůřek** and his company RaRe Technologies.\n",
        "\n",
        "It is not an everything-including-the-kitchen-sink NLP research library (like NLTK); instead, Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling. Most notably for this tutorial, it supports an implementation of the** Word2Vec word embedding** for learning new word vectors from text.\n",
        "\n",
        "It also provides tools for loading pre-trained word embeddings in a few formats and for making use and querying a loaded embedding.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this tutorial, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and **semantic relationships** among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4OuxlUSuPy7"
      },
      "source": [
        "**Please install & import the gensim everytime you work on Google colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16M51r6GOuFC"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgGRlg9MuehZ"
      },
      "source": [
        "**Since we are going to work with words, so we are required to split the each review so that we can have word tokens.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtVP0tOb-fJL"
      },
      "source": [
        "# Word tokenization\n",
        "Document=[]\n",
        "for doc in df['clean_review']:\n",
        "  Document.append(doc.split(' '))                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tta5zrdnrlft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c8fbfb-1267-4d22-c421-bf8cfdfae73d"
      },
      "source": [
        "len(Document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heF-_9pObZiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfp2mtdQu8oU"
      },
      "source": [
        "**Let us explore split reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV4hRqzwVt4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cfda31-c485-438f-a4b9-aed13e511d42"
      },
      "source": [
        "Document[10][6:13]                                                                # This what is there in 10th Document starting from 6 till 12"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movie', 'i', 'am', 'not', 'sure', 'whether', 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWDFXrYVt7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba55e9b-d02b-4361-e090-e7333fc2ef5e"
      },
      "source": [
        "print(len(Document[10]))                                                          # Lenth of the 10th document ,  It has 524 words in it\n",
        "print(Document[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "524\n",
            "['after', 'reading', 'the', 'comments', 'for', 'this', 'movie', 'i', 'am', 'not', 'sure', 'whether', 'i', 'should', 'be', 'angry', 'sad', 'or', 'sickened', 'seeing', 'comments', 'typical', 'of', 'people', 'who', 'a', 'know', 'absolutely', 'nothing', 'about', 'the', 'military', 'or', 'b', 'who', 'base', 'everything', 'they', 'think', 'they', 'know', 'on', 'movies', 'like', 'this', 'or', 'on', 'cnn', 'reports', 'about', 'abu', 'gharib', 'makes', 'me', 'wonder', 'about', 'the', 'state', 'of', 'intellectual', 'stimulation', 'in', 'the', 'world', 'br', 'br', 'at', 'the', 'time', 'i', 'type', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'us', 'military', 'million', 'on', 'active', 'duty', 'with', 'another', 'almost', 'in', 'the', 'guard', 'and', 'reserves', 'for', 'a', 'total', 'of', 'roughly', 'million', 'br', 'br', 'the', 'number', 'of', 'people', 'indicted', 'for', 'abuses', 'at', 'at', 'abu', 'gharib', 'currently', 'less', 'than', 'br', 'br', 'that', 'makes', 'the', 'total', 'of', 'people', 'indicted', 'of', 'the', 'total', 'military', 'even', 'if', 'you', 'indict', 'every', 'single', 'military', 'member', 'that', 'ever', 'stepped', 'in', 'to', 'abu', 'gharib', 'you', 'would', 'not', 'come', 'close', 'to', 'making', 'that', 'a', 'whole', 'number', 'br', 'br', 'the', 'flaws', 'in', 'this', 'movie', 'would', 'take', 'years', 'to', 'cover', 'i', 'understand', 'that', 'it', 's', 'supposed', 'to', 'be', 'sarcastic', 'but', 'in', 'reality', 'the', 'writer', 'and', 'director', 'are', 'trying', 'to', 'make', 'commentary', 'about', 'the', 'state', 'of', 'the', 'military', 'without', 'an', 'enemy', 'to', 'fight', 'in', 'reality', 'the', 'us', 'military', 'has', 'been', 'at', 'its', 'busiest', 'when', 'there', 'are', 'not', 'conflicts', 'going', 'on', 'the', 'military', 'is', 'the', 'first', 'called', 'for', 'disaster', 'relief', 'and', 'humanitarian', 'aid', 'missions', 'when', 'the', 'tsunami', 'hit', 'indonesia', 'devestating', 'the', 'region', 'the', 'us', 'military', 'was', 'the', 'first', 'on', 'the', 'scene', 'when', 'the', 'chaos', 'of', 'the', 'situation', 'overwhelmed', 'the', 'local', 'governments', 'it', 'was', 'military', 'leadership', 'who', 'looked', 'at', 'their', 'people', 'the', 'same', 'people', 'this', 'movie', 'mocks', 'and', 'said', 'make', 'it', 'happen', 'within', 'hours', 'food', 'aid', 'was', 'reaching', 'isolated', 'villages', 'within', 'days', 'airfields', 'were', 'built', 'cargo', 'aircraft', 'started', 'landing', 'and', 'a', 'food', 'distribution', 'system', 'was', 'up', 'and', 'running', 'hours', 'and', 'days', 'not', 'weeks', 'and', 'months', 'yes', 'there', 'are', 'unscrupulous', 'people', 'in', 'the', 'us', 'military', 'but', 'then', 'there', 'are', 'in', 'every', 'walk', 'of', 'life', 'every', 'occupation', 'but', 'to', 'see', 'people', 'on', 'this', 'website', 'decide', 'that', 'million', 'men', 'and', 'women', 'are', 'all', 'criminal', 'with', 'nothing', 'on', 'their', 'minds', 'but', 'thoughts', 'of', 'destruction', 'or', 'mayhem', 'is', 'an', 'absolute', 'disservice', 'to', 'the', 'things', 'that', 'they', 'do', 'every', 'day', 'one', 'person', 'on', 'this', 'website', 'even', 'went', 'so', 'far', 'as', 'to', 'say', 'that', 'military', 'members', 'are', 'in', 'it', 'for', 'personal', 'gain', 'wow', 'entry', 'level', 'personnel', 'make', 'just', 'under', 'an', 'hour', 'assuming', 'a', 'hour', 'work', 'week', 'of', 'course', 'many', 'work', 'much', 'more', 'than', 'hours', 'a', 'week', 'and', 'those', 'in', 'harm', 's', 'way', 'typically', 'put', 'in', 'hour', 'days', 'for', 'months', 'on', 'end', 'that', 'makes', 'the', 'pay', 'well', 'under', 'minimum', 'wage', 'so', 'much', 'for', 'personal', 'gain', 'i', 'beg', 'you', 'please', 'make', 'yourself', 'familiar', 'with', 'the', 'world', 'around', 'you', 'go', 'to', 'a', 'nearby', 'base', 'get', 'a', 'visitor', 'pass', 'and', 'meet', 'some', 'of', 'the', 'men', 'and', 'women', 'you', 'are', 'so', 'quick', 'to', 'disparage', 'you', 'would', 'be', 'surprised', 'the', 'military', 'no', 'longer', 'accepts', 'people', 'in', 'lieu', 'of', 'prison', 'time', 'they', 'require', 'a', 'minimum', 'of', 'a', 'ged', 'and', 'prefer', 'a', 'high', 'school', 'diploma', 'the', 'middle', 'ranks', 'are', 'expected', 'to', 'get', 'a', 'minimum', 'of', 'undergraduate', 'degrees', 'and', 'the', 'upper', 'ranks', 'are', 'encouraged', 'to', 'get', 'advanced', 'degrees']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jamR9vRCPJM-"
      },
      "source": [
        "import logging                                                                    # Please import logging to keep & check information regarding word2vec transformation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fdKWAs2VuAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffc278e-43f5-432a-8b78-76ef784977d2"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "model=gensim.models.Word2Vec(Document,                                           # List of reviews\n",
        "                          min_count=10,                                          # we want words appearing atleast 10 times in the vocab otherwise ignore min_df\n",
        "                          workers=4,                                             # Use these many worker threads to train the model (=faster training with multicore machines\n",
        "                           size=50,                                              # it means aword is represented by 50 numbers,in other words the number of neorons in hidden layer is 50 \n",
        "                          window=5)                                              # 5 neighbors on the either side of a word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 09:23:30,428 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2021-04-09 09:23:30,430 : INFO : collecting all words and their counts\n",
            "2021-04-09 09:23:30,432 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-04-09 09:23:31,009 : INFO : PROGRESS: at sentence #10000, processed 2399440 words, keeping 51654 word types\n",
            "2021-04-09 09:23:31,622 : INFO : PROGRESS: at sentence #20000, processed 4835846 words, keeping 69077 word types\n",
            "2021-04-09 09:23:32,196 : INFO : PROGRESS: at sentence #30000, processed 7267977 words, keeping 81515 word types\n",
            "2021-04-09 09:23:32,789 : INFO : PROGRESS: at sentence #40000, processed 9669772 words, keeping 91685 word types\n",
            "2021-04-09 09:23:33,370 : INFO : collected 100479 word types from a corpus of 12084660 raw words and 50000 sentences\n",
            "2021-04-09 09:23:33,372 : INFO : Loading a fresh vocabulary\n",
            "2021-04-09 09:23:33,781 : INFO : effective_min_count=10 retains 28322 unique words (28% of original 100479, drops 72157)\n",
            "2021-04-09 09:23:33,785 : INFO : effective_min_count=10 leaves 11910457 word corpus (98% of original 12084660, drops 174203)\n",
            "2021-04-09 09:23:33,888 : INFO : deleting the raw counts dictionary of 100479 items\n",
            "2021-04-09 09:23:33,895 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2021-04-09 09:23:33,897 : INFO : downsampling leaves estimated 8817283 word corpus (74.0% of prior 11910457)\n",
            "2021-04-09 09:23:34,020 : INFO : estimated required memory for 28322 words and 50 dimensions: 25489800 bytes\n",
            "2021-04-09 09:23:34,024 : INFO : resetting layer weights\n",
            "2021-04-09 09:23:40,050 : INFO : training model with 4 workers on 28322 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2021-04-09 09:23:41,072 : INFO : EPOCH 1 - PROGRESS: at 5.12% examples, 445526 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:42,075 : INFO : EPOCH 1 - PROGRESS: at 10.40% examples, 452057 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:43,076 : INFO : EPOCH 1 - PROGRESS: at 15.77% examples, 457049 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:44,086 : INFO : EPOCH 1 - PROGRESS: at 21.05% examples, 457657 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:45,095 : INFO : EPOCH 1 - PROGRESS: at 26.31% examples, 458415 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:46,109 : INFO : EPOCH 1 - PROGRESS: at 31.53% examples, 457257 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:23:47,140 : INFO : EPOCH 1 - PROGRESS: at 36.75% examples, 456334 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:48,193 : INFO : EPOCH 1 - PROGRESS: at 42.02% examples, 455401 words/s, in_qsize 8, out_qsize 3\n",
            "2021-04-09 09:23:49,217 : INFO : EPOCH 1 - PROGRESS: at 47.44% examples, 457624 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:50,252 : INFO : EPOCH 1 - PROGRESS: at 52.80% examples, 456957 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:51,276 : INFO : EPOCH 1 - PROGRESS: at 58.03% examples, 456983 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:52,276 : INFO : EPOCH 1 - PROGRESS: at 63.13% examples, 457107 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:53,277 : INFO : EPOCH 1 - PROGRESS: at 68.30% examples, 456603 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-09 09:23:54,293 : INFO : EPOCH 1 - PROGRESS: at 73.58% examples, 456270 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:23:55,311 : INFO : EPOCH 1 - PROGRESS: at 79.07% examples, 457321 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:56,318 : INFO : EPOCH 1 - PROGRESS: at 84.33% examples, 457637 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:57,332 : INFO : EPOCH 1 - PROGRESS: at 89.85% examples, 458564 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:58,346 : INFO : EPOCH 1 - PROGRESS: at 95.17% examples, 458942 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:23:59,249 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-09 09:23:59,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-09 09:23:59,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-09 09:23:59,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-09 09:23:59,288 : INFO : EPOCH - 1 : training on 12084660 raw words (8816600 effective words) took 19.2s, 458515 effective words/s\n",
            "2021-04-09 09:24:00,328 : INFO : EPOCH 2 - PROGRESS: at 5.04% examples, 430754 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-09 09:24:01,380 : INFO : EPOCH 2 - PROGRESS: at 10.64% examples, 447661 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:02,391 : INFO : EPOCH 2 - PROGRESS: at 16.00% examples, 452658 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:03,412 : INFO : EPOCH 2 - PROGRESS: at 21.30% examples, 453082 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:04,435 : INFO : EPOCH 2 - PROGRESS: at 26.68% examples, 454841 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:05,447 : INFO : EPOCH 2 - PROGRESS: at 31.97% examples, 456863 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:06,474 : INFO : EPOCH 2 - PROGRESS: at 37.34% examples, 458357 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-09 09:24:07,491 : INFO : EPOCH 2 - PROGRESS: at 42.74% examples, 460020 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:08,492 : INFO : EPOCH 2 - PROGRESS: at 48.04% examples, 461305 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:09,504 : INFO : EPOCH 2 - PROGRESS: at 53.28% examples, 460678 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:10,519 : INFO : EPOCH 2 - PROGRESS: at 58.52% examples, 460676 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:11,520 : INFO : EPOCH 2 - PROGRESS: at 63.47% examples, 459296 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:12,546 : INFO : EPOCH 2 - PROGRESS: at 68.80% examples, 458887 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:13,555 : INFO : EPOCH 2 - PROGRESS: at 74.04% examples, 458621 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:14,590 : INFO : EPOCH 2 - PROGRESS: at 79.56% examples, 459023 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:15,600 : INFO : EPOCH 2 - PROGRESS: at 84.80% examples, 459156 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:16,605 : INFO : EPOCH 2 - PROGRESS: at 90.26% examples, 459828 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:17,608 : INFO : EPOCH 2 - PROGRESS: at 95.46% examples, 459985 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:18,424 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-09 09:24:18,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-09 09:24:18,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-09 09:24:18,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-09 09:24:18,453 : INFO : EPOCH - 2 : training on 12084660 raw words (8819223 effective words) took 19.2s, 460392 effective words/s\n",
            "2021-04-09 09:24:19,518 : INFO : EPOCH 3 - PROGRESS: at 5.28% examples, 441506 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-09 09:24:20,521 : INFO : EPOCH 3 - PROGRESS: at 10.56% examples, 450078 words/s, in_qsize 8, out_qsize 2\n",
            "2021-04-09 09:24:21,525 : INFO : EPOCH 3 - PROGRESS: at 15.85% examples, 452829 words/s, in_qsize 5, out_qsize 1\n",
            "2021-04-09 09:24:22,532 : INFO : EPOCH 3 - PROGRESS: at 21.13% examples, 455004 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:23,541 : INFO : EPOCH 3 - PROGRESS: at 26.40% examples, 456187 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-09 09:24:24,561 : INFO : EPOCH 3 - PROGRESS: at 31.68% examples, 456169 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:25,580 : INFO : EPOCH 3 - PROGRESS: at 37.07% examples, 458241 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:26,630 : INFO : EPOCH 3 - PROGRESS: at 42.39% examples, 458073 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-09 09:24:27,651 : INFO : EPOCH 3 - PROGRESS: at 47.69% examples, 458561 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:28,673 : INFO : EPOCH 3 - PROGRESS: at 53.20% examples, 459809 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:29,684 : INFO : EPOCH 3 - PROGRESS: at 58.36% examples, 459448 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:30,697 : INFO : EPOCH 3 - PROGRESS: at 63.48% examples, 458919 words/s, in_qsize 4, out_qsize 3\n",
            "2021-04-09 09:24:31,720 : INFO : EPOCH 3 - PROGRESS: at 69.02% examples, 460210 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:32,734 : INFO : EPOCH 3 - PROGRESS: at 74.20% examples, 459169 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-09 09:24:33,737 : INFO : EPOCH 3 - PROGRESS: at 79.74% examples, 460449 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:34,765 : INFO : EPOCH 3 - PROGRESS: at 84.96% examples, 460009 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:35,778 : INFO : EPOCH 3 - PROGRESS: at 90.35% examples, 460312 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-09 09:24:36,769 : INFO : EPOCH 3 - PROGRESS: at 95.54% examples, 460413 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:37,532 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-09 09:24:37,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-09 09:24:37,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-09 09:24:37,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-09 09:24:37,570 : INFO : EPOCH - 3 : training on 12084660 raw words (8817870 effective words) took 19.1s, 461514 effective words/s\n",
            "2021-04-09 09:24:38,605 : INFO : EPOCH 4 - PROGRESS: at 5.21% examples, 446302 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:39,610 : INFO : EPOCH 4 - PROGRESS: at 10.64% examples, 459351 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:40,620 : INFO : EPOCH 4 - PROGRESS: at 16.01% examples, 460489 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:41,621 : INFO : EPOCH 4 - PROGRESS: at 21.14% examples, 457720 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:42,637 : INFO : EPOCH 4 - PROGRESS: at 26.24% examples, 454961 words/s, in_qsize 7, out_qsize 3\n",
            "2021-04-09 09:24:43,639 : INFO : EPOCH 4 - PROGRESS: at 31.54% examples, 456484 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:44,665 : INFO : EPOCH 4 - PROGRESS: at 36.61% examples, 453956 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:45,665 : INFO : EPOCH 4 - PROGRESS: at 41.78% examples, 455359 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:46,674 : INFO : EPOCH 4 - PROGRESS: at 46.96% examples, 456016 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:47,675 : INFO : EPOCH 4 - PROGRESS: at 52.24% examples, 456352 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:48,702 : INFO : EPOCH 4 - PROGRESS: at 57.40% examples, 455651 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-09 09:24:49,749 : INFO : EPOCH 4 - PROGRESS: at 62.81% examples, 456457 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:50,756 : INFO : EPOCH 4 - PROGRESS: at 67.98% examples, 455798 words/s, in_qsize 8, out_qsize 2\n",
            "2021-04-09 09:24:51,771 : INFO : EPOCH 4 - PROGRESS: at 73.32% examples, 456057 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:52,773 : INFO : EPOCH 4 - PROGRESS: at 78.44% examples, 455692 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:53,810 : INFO : EPOCH 4 - PROGRESS: at 83.93% examples, 456158 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:54,820 : INFO : EPOCH 4 - PROGRESS: at 89.18% examples, 455634 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:24:55,825 : INFO : EPOCH 4 - PROGRESS: at 94.50% examples, 456806 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-09 09:24:56,808 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-09 09:24:56,832 : INFO : EPOCH 4 - PROGRESS: at 99.84% examples, 457171 words/s, in_qsize 2, out_qsize 1\n",
            "2021-04-09 09:24:56,834 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-09 09:24:56,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-09 09:24:56,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-09 09:24:56,856 : INFO : EPOCH - 4 : training on 12084660 raw words (8816352 effective words) took 19.3s, 457353 effective words/s\n",
            "2021-04-09 09:24:57,875 : INFO : EPOCH 5 - PROGRESS: at 5.12% examples, 447039 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:58,886 : INFO : EPOCH 5 - PROGRESS: at 10.40% examples, 450813 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:24:59,892 : INFO : EPOCH 5 - PROGRESS: at 15.70% examples, 453107 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:25:00,895 : INFO : EPOCH 5 - PROGRESS: at 20.87% examples, 453694 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:01,938 : INFO : EPOCH 5 - PROGRESS: at 26.09% examples, 451266 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-09 09:25:02,936 : INFO : EPOCH 5 - PROGRESS: at 31.29% examples, 452182 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:25:03,950 : INFO : EPOCH 5 - PROGRESS: at 36.16% examples, 448035 words/s, in_qsize 4, out_qsize 3\n",
            "2021-04-09 09:25:04,960 : INFO : EPOCH 5 - PROGRESS: at 41.39% examples, 450501 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:05,979 : INFO : EPOCH 5 - PROGRESS: at 46.38% examples, 449602 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:06,990 : INFO : EPOCH 5 - PROGRESS: at 51.81% examples, 451436 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:08,000 : INFO : EPOCH 5 - PROGRESS: at 56.82% examples, 450572 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:09,009 : INFO : EPOCH 5 - PROGRESS: at 61.89% examples, 450428 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-09 09:25:10,039 : INFO : EPOCH 5 - PROGRESS: at 67.02% examples, 449918 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:11,050 : INFO : EPOCH 5 - PROGRESS: at 72.33% examples, 450245 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:12,077 : INFO : EPOCH 5 - PROGRESS: at 77.41% examples, 449518 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:25:13,109 : INFO : EPOCH 5 - PROGRESS: at 82.85% examples, 450021 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:14,120 : INFO : EPOCH 5 - PROGRESS: at 87.76% examples, 448201 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:25:15,139 : INFO : EPOCH 5 - PROGRESS: at 93.02% examples, 448710 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-09 09:25:16,148 : INFO : EPOCH 5 - PROGRESS: at 98.16% examples, 448916 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-09 09:25:16,467 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-09 09:25:16,477 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-09 09:25:16,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-09 09:25:16,486 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-09 09:25:16,490 : INFO : EPOCH - 5 : training on 12084660 raw words (8816409 effective words) took 19.6s, 449253 effective words/s\n",
            "2021-04-09 09:25:16,493 : INFO : training on a 60423300 raw words (44086454 effective words) took 96.4s, 457133 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHlVJEoXC5PD"
      },
      "source": [
        "**Please note that after applying Word2Vec function on the clean_review giving all the arguments corretly we have got 28322 words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RYnV85LCq3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8c0067-c45f-4756-b29b-d4553866b3bb"
      },
      "source": [
        "print(len(model.wv.vocab))                                                        # Now the vocab contains 28322 uinque words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdk_HDQWD0ct"
      },
      "source": [
        "**Let's check the dimension of a vector i.e. the number of words that represent a word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DGahs80Dv6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c864a6-62df-4d79-e8b8-f85df214b939"
      },
      "source": [
        "print(model.wv.vector_size)                                                       # It means each vector has 50 numbers in it or in other words each word is vector of 5o numbers that we predefined"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmrT73MQsgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e4c199-714d-4c00-bdb6-cc49140d726b"
      },
      "source": [
        "model.wv.vectors.shape                                                            # Dimension of the the entire corpus        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28322, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyTej6dydwW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb3da63-c6e9-44c1-e351-961e92d1eb8a"
      },
      "source": [
        "model['beautiful'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGHd2OREPSz"
      },
      "source": [
        "### Let's explore some interesting results of word2vec experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db44U_SoVuC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65d3caf-9bb3-4381-9145-eb53f69491da"
      },
      "source": [
        "model.wv.most_similar(\"beautiful\")                                                # 10 similar words beautiful,the maximum similarity is 1,minimum is 0.When they are completely similar the \n",
        "                                                                                  # Value will be 1 , when completely dissimilar,the value will be 0."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 09:26:13,854 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gorgeous', 0.8588871359825134),\n",
              " ('lovely', 0.833928108215332),\n",
              " ('stunning', 0.8132712244987488),\n",
              " ('wonderful', 0.7337294816970825),\n",
              " ('haunting', 0.7228685021400452),\n",
              " ('breathtaking', 0.6874433755874634),\n",
              " ('delightful', 0.6828726530075073),\n",
              " ('fabulous', 0.6743853092193604),\n",
              " ('beauty', 0.6634806394577026),\n",
              " ('exquisite', 0.663422167301178)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NbHKeQg6kP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bed41e-6958-4384-cfbd-2c6f81fb6651"
      },
      "source": [
        "model.wv.most_similar(\"princess\")                                                  # 10 similar words returned with numbers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('widow', 0.8474273085594177),\n",
              " ('maria', 0.8233842849731445),\n",
              " ('maid', 0.8153141736984253),\n",
              " ('nurse', 0.8119286894798279),\n",
              " ('prince', 0.801642894744873),\n",
              " ('daisy', 0.7929926514625549),\n",
              " ('queen', 0.7837755084037781),\n",
              " ('belle', 0.7829538583755493),\n",
              " ('servant', 0.7782474756240845),\n",
              " ('virgin', 0.7725341320037842)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-tDNwJD-fQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "91f0b5ff-fdff-4901-c107-5d1517f8f047"
      },
      "source": [
        "model.wv.doesnt_match(\"she richard talked to me in the evening publicly\".split())         # publicly does not match in the sentence given"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'richard'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3lClNUZFpQE"
      },
      "source": [
        "Below the word **right** is represented by a dense 50 dimensional vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sj-zNbJ6kKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "ef92ba35-9b51-4e0b-b3aa-3c873ad0df18"
      },
      "source": [
        "model.wv[\"right\"]                                                                  # right word is represented by 50 numbers in other words the word \"right\" is vector of 50 numbers\n",
        "                                                                                   # 50 numbers are summarized weights because these numbers are obtained in the hidden layer of predefined 50 neurons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.6846868 , -0.7968005 , -0.6302324 , -3.394566  ,  0.7201638 ,\n",
              "        1.1136231 ,  0.15784374,  2.5323277 ,  3.5603287 ,  0.59281504,\n",
              "       -0.9188659 , -0.447955  ,  0.62603706, -0.03631172,  1.1669568 ,\n",
              "        1.2037969 , -1.878727  ,  1.026431  ,  1.2620952 ,  0.87099075,\n",
              "       -0.40098897, -2.2058098 , -2.5231655 , -0.3569199 , -1.5878537 ,\n",
              "       -1.0750184 ,  0.15629587, -0.23911397, -1.4470414 , -0.1245377 ,\n",
              "        1.136354  , -0.6609408 ,  0.5265115 ,  0.6114063 , -0.15428022,\n",
              "       -0.1421413 , -0.569659  ,  2.165041  , -2.8660035 ,  0.5492531 ,\n",
              "        0.93151784,  0.21207607,  2.0477517 ,  1.5115618 ,  0.76157683,\n",
              "       -1.3251568 ,  0.78988296,  0.98710644,  1.5385407 ,  3.4042273 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnENAcilu0pP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "477da5bc-bb26-4c26-cebc-15f854841fab"
      },
      "source": [
        "model.wv['great']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.4401122 ,  0.76371634, -1.4465237 , -2.4345956 ,  1.4818008 ,\n",
              "        2.3014462 ,  0.35223305, -0.28248826,  1.3010585 ,  0.46277562,\n",
              "       -0.43015203,  0.64026016,  0.26170298,  0.63460237,  2.4360445 ,\n",
              "        1.3040127 ,  1.5610905 , -2.5930145 , -0.68386453,  1.2764063 ,\n",
              "       -2.0910609 , -0.79727757, -1.6921145 ,  0.81008387, -1.4320858 ,\n",
              "        2.696001  , -0.83759457, -1.203519  ,  0.7344471 ,  4.284749  ,\n",
              "       -0.97111064, -0.95781547,  5.1075478 , -0.13110286, -1.0417333 ,\n",
              "        1.0495613 ,  0.5642384 ,  4.2558985 , -3.4282224 , -0.5562852 ,\n",
              "        2.6257257 ,  0.06977661, -0.5748929 ,  2.0517044 ,  1.0178621 ,\n",
              "       -0.41138136, -1.5831146 ,  2.879354  , -0.30539533,  2.2108846 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWdfqY5QuPRa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "4c4b89d7-4535-4979-8ea8-f68363de9d41"
      },
      "source": [
        "model.wv."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-518bbf67b2e3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    model.wv.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CItdPnnHGk66"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MMsUOIp-fNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "8febec39-9223-4ce7-84be-5724ce534a70"
      },
      "source": [
        "model.save(\"word2vec movie-50\")                                                    # We save this model for further use.\n",
        "                                                                                   # Google has such many pre-trained models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-19 05:41:08,396 : INFO : saving Word2Vec object under word2vec movie-50, separately None\n",
            "2020-01-19 05:41:08,398 : INFO : not storing attribute vectors_norm\n",
            "2020-01-19 05:41:08,404 : INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-01-19 05:41:08,692 : INFO : saved word2vec movie-50\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a1G2kw_6kf2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAVyObUcLqV9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}